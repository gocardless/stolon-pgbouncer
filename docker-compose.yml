---
version: "3.7"

services:
  etcd-store:
    image: quay.io/coreos/etcd:v3.2.17
    restart: on-failure
    entrypoint:
      - etcd
    command:
      - --data-dir=/data
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://0.0.0.0:2379
    volumes:
      - etcd-store-data:/data
    ports:
      - "2379:2379"
    networks:
      default:
        aliases:
          - etcd-store

  sentinel:
    image: &stolonDevelopmentImage gocardless/stolon-development:2022030901
    build: &stolonDevelopmentBuild
      context: docker/stolon-development
    restart: on-failure
    depends_on:
      - etcd-store
    entrypoint:
      - /usr/local/bin/stolon-sentinel
      - --cluster-name=main
      - --store-backend=etcdv3
      - --store-endpoints=etcd-store:2379
      - --metrics-listen-address=0.0.0.0:9459
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9459/metrics"]

  pgbouncer:
    hostname: pgbouncer
    image: *stolonDevelopmentImage
    build: *stolonDevelopmentBuild
    command:
      - /stolon-pgbouncer/docker/stolon-development/supervisord-pgbouncer.conf
    restart: on-failure
    volumes:
      - .:/stolon-pgbouncer
    ports:
      - "6432:6432"
    depends_on:
      keeper0:
        condition: service_healthy
      keeper1:
        condition: service_healthy
      keeper2:
        condition: service_healthy

  keeper0:
    hostname: keeper0
    image: *stolonDevelopmentImage
    build: *stolonDevelopmentBuild
    command:
      - /stolon-pgbouncer/docker/stolon-development/supervisord.conf
    restart: on-failure
    volumes:
      - keeper0-data:/data
      - keeper0-wal:/wal
      - .:/stolon-pgbouncer
    environment:
      - WAL_DIR=/wal
    ports:
      - "6433:6432"
    depends_on:
      sentinel:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/usr/bin/pg_isready", "-h", "/tmp"]

  keeper1:
    hostname: keeper1
    image: *stolonDevelopmentImage
    build: *stolonDevelopmentBuild
    command:
      - /stolon-pgbouncer/docker/stolon-development/supervisord.conf
    restart: on-failure
    volumes:
      - keeper1-data:/data
      - keeper1-wal:/wal
      - .:/stolon-pgbouncer
    environment:
      - WAL_DIR=/wal
    ports:
      - "6434:6432"
    depends_on:
      sentinel:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/usr/bin/pg_isready", "-h", "/tmp"]

  keeper2:
    hostname: keeper2
    image: *stolonDevelopmentImage
    build: *stolonDevelopmentBuild
    command:
      - /stolon-pgbouncer/docker/stolon-development/supervisord.conf
    restart: on-failure
    volumes:
      - keeper2-data:/data
      - .:/stolon-pgbouncer
    ports:
      - "6435:6432"
    depends_on:
      sentinel:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/usr/bin/pg_isready", "-h", "/tmp"]

  prometheus:
    image: prom/prometheus
    volumes:
      - ./docker/observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./docker/observability/prometheus/rules.yml:/etc/prometheus/rules.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: always
    ports:
      - 9090:9090
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9090"]

  grafana:
    image: grafana/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    volumes:
      - ./docker/observability/grafana/dashboards:/var/lib/grafana/dashboards
      - ./docker/observability/grafana/dashboard-provisioner.yml:/etc/grafana/provisioning/dashboards/provisioner.yml
      - ./docker/observability/grafana/datasource-provisioner.yml:/etc/grafana/provisioning/datasources/provisioner.yml
      - grafana-data:/var/lib/grafana
    user: "472"
    ports:
      - 3000:3000
    restart: always

# Persist etcd and keeper data across docker restarts. This enables our cluster
# to outlive killing our containers, as the keepers otherwise complain that
# their data went missing.
volumes:
  etcd-store-data:
  keeper0-data:
  keeper1-data:
  keeper2-data:
  keeper0-wal:
  keeper1-wal:
  prometheus-data:
  grafana-data:

networks:
  default:
    driver: bridge
